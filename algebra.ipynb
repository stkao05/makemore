{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34953c51-113f-4638-a085-78ec2bce7b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe6ab3c-0b78-471e-858a-dcfcd913b79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_char = \";\"\n",
    "pad_char = \"_\"\n",
    "chars = list(\"_0123456789+=;\")\n",
    "vocab_size = len(chars)\n",
    "ctoi = {c:i for i, c in enumerate(chars)}\n",
    "itoc = {i:c for i, c in enumerate(chars)}\n",
    "\n",
    "eq_token = ctoi[\"=\"]\n",
    "stop_token = ctoi[\";\"]\n",
    "\n",
    "# return list of integer\n",
    "def encode(string):\n",
    "    return [ctoi[c] for c in string] \n",
    "\n",
    "def decode(tokens):\n",
    "    if isinstance(tokens, torch.Tensor):\n",
    "        return \"\".join([itoc[t.item()] for t in tokens])\n",
    "    else:\n",
    "        return \"\".join([itoc[t] for t in tokens])\n",
    "\n",
    "ctoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c915bbcd-1201-4a31-bb94-7bfbc5a4d8e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'39+31=07;'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_eq(hi_range=100):\n",
    "    a = torch.randint(0, hi_range, (1,))\n",
    "    b = torch.randint(0, hi_range, (1,))\n",
    "    c = a + b\n",
    "    cs = f\"{c.item()}\"[::-1]\n",
    "    return f\"{a.item()}+{b.item()}={cs};\"\n",
    "\n",
    "generate_eq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "eda891ba-e2a4-4d7b-bf75-a0ea895e7fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 0, 0, 0]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad(ints, length):\n",
    "    assert length >= len(ints)\n",
    "    pn = length - len(ints)\n",
    "    return ints + [0]*pn\n",
    "        \n",
    "pad([1,2], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ed4983d6-75e2-45e0-bdcb-8f2afbd2df62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 4,  4, 11, 10,  2, 12,  5,  3,  2, 13,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 2,  2, 11,  9,  7, 12,  8, 10, 13,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]),\n",
       " tensor([[ 0,  0,  0,  0,  0,  0,  5,  3,  2, 13,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  8, 10, 13,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 32\n",
    "padding = 0\n",
    "\n",
    "# (B, L)\n",
    "def random_batch(batch_size, hi_range=100):\n",
    "    eq_str = [generate_eq(hi_range) for _ in range(batch_size)]\n",
    "    data = [pad(encode(s), block_size) for s in eq_str]\n",
    "    target = []\n",
    "    \n",
    "    for x in data:\n",
    "        y = list(x)\n",
    "        i = x.index(eq_token)\n",
    "        y[0:i+1] = [0]*(i+1)\n",
    "        target.append(y)\n",
    "        \n",
    "    return torch.tensor(data), torch.tensor(target)\n",
    "\n",
    "\n",
    "x, y = random_batch(2)\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a6d4a9bf-dda6-4171-993c-0cc3506e7382",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    vocab_size: int\n",
    "    block_size: int\n",
    "    emb_size: int\n",
    "    head_num: int\n",
    "    head_size: int\n",
    "    layer_num: int\n",
    "    ctoi: dict\n",
    "    dropout: float\n",
    "\n",
    "\n",
    "class MultiHeadAttension(nn.Module):\n",
    "\n",
    "    def __init__(self, c: Config):\n",
    "        super().__init__()\n",
    "        assert c.emb_size / c.head_size == c.head_num\n",
    "\n",
    "        self.head_size = c.head_size\n",
    "        self.head_num = c.head_num\n",
    "        self.attn = nn.Linear(\n",
    "            c.emb_size, 3 * c.head_num * c.head_size, bias=False)\n",
    "        self.ffn = nn.Linear(c.head_num * c.head_size, c.emb_size, bias=False)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(c.dropout)\n",
    "        self.resid_dropout = nn.Dropout(c.dropout)\n",
    "\n",
    "    # x: (B, L, C)\n",
    "    # return: (B, L, C)\n",
    "    def forward(self, x):\n",
    "        B, L, C = x.shape\n",
    "\n",
    "        z = self.attn(x)  # (B, L, 3 * hn * hs)\n",
    "        k, q, v = torch.split(\n",
    "            z, self.head_num * self.head_size, dim=2)  # (B, L, hn * hs)\n",
    "\n",
    "        k = k.view(B, L, self.head_num, self.head_size).permute(\n",
    "            0, 2, 1, 3)  # (B, hn, L, hs)\n",
    "        q = q.view(B, L, self.head_num, self.head_size).permute(0, 2, 1, 3)\n",
    "        v = v.view(B, L, self.head_num, self.head_size).permute(0, 2, 1, 3)\n",
    "\n",
    "        q = q.permute(0, 1, 3, 2)  # (B, hn, hs, L)\n",
    "        attn = (k @ q) / self.head_size**0.5  # (B, hn, L, L)\n",
    "        mask = torch.tril(torch.ones(L, L)) == 0\n",
    "        mask = mask.to(x.device)\n",
    "        attn = attn.masked_fill(mask, -float('inf'))  # (B, hn, L, L)\n",
    "        attn = F.softmax(attn, dim=3)\n",
    "        attn = self.attn_dropout(attn)\n",
    "\n",
    "        y = attn @ v  # (B, hn, L, hs)\n",
    "        y = y.permute(0, 2, 1, 3)  # (B, L, hn, hs)\n",
    "        y = y.contiguous().view(B, L, -1)  # (B, L, hn * hs)\n",
    "        y = self.ffn(y)  # (B, L, C)\n",
    "        y = self.resid_dropout(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, c: Config):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(c.emb_size, 2 * c.emb_size)\n",
    "        self.linear2 = nn.Linear(2 * c.emb_size, c.emb_size)\n",
    "        self.dropout = nn.Dropout(c.dropout)\n",
    "\n",
    "    # (B, L, C)\n",
    "    def forward(self, x):\n",
    "        y = self.linear1(x)\n",
    "        y = torch.relu(y)\n",
    "        y = self.linear2(y)\n",
    "        y = self.dropout(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, c: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        assert c.emb_size % c.head_size == 0\n",
    "        assert c.emb_size / c.head_size == c.head_num\n",
    "\n",
    "        self.mha = MultiHeadAttension(c)\n",
    "        self.lnorm1 = nn.LayerNorm(c.emb_size)\n",
    "        self.lnorm2 = nn.LayerNorm(c.emb_size)\n",
    "        self.ffn = FeedForward(c)\n",
    "\n",
    "    # x: (B, L, emb)\n",
    "    def forward(self, x):\n",
    "        y = self.mha(x) + x\n",
    "        y = self.lnorm1(y)\n",
    "        y = self.ffn(y) + y\n",
    "        y = self.lnorm2(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, c: Config):\n",
    "        super().__init__()\n",
    "        self.config = c\n",
    "        self.embed = nn.Embedding(c.vocab_size, c.emb_size)\n",
    "        self.dropout = nn.Dropout(c.dropout)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(c) for _ in range(c.layer_num)]\n",
    "        )\n",
    "        self.proj = nn.Linear(c.emb_size, c.vocab_size)\n",
    "\n",
    "    # return (L, C)\n",
    "    def pos_encoding(self, x):\n",
    "        B, L, C = x.shape\n",
    "        pos = torch.arange(0, L).view(-1, 1)  # (L, 1)\n",
    "        div = 2 * torch.arange(0, C) / C  # (C)\n",
    "        div = torch.pow(10000, div)  # (C)\n",
    "        e = pos / div\n",
    "        pe = torch.zeros(L, C)\n",
    "        pe[:, 0::2] = torch.sin(e[:, 0::2])\n",
    "        pe[:, 1::2] = torch.cos(e[:, 1::2])\n",
    "\n",
    "        pe = pe.to(x.device)\n",
    "        return pe\n",
    "\n",
    "    # (B, L) -> (B, L, C)\n",
    "    def forward(self, x):\n",
    "        y = self.embed(x)  # (B, L, emb)\n",
    "        y = y + self.pos_encoding(y)  # (B, L, emb)\n",
    "        y = self.dropout(y)\n",
    "        y = self.blocks(y)  # (B, L, emb)\n",
    "        y = self.proj(y)  # (B, L, vocab)\n",
    "\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "39fb72d7-33b0-4d53-a26c-83780b301c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34574"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = Config(\n",
    "    vocab_size = vocab_size,\n",
    "    block_size=block_size,\n",
    "    emb_size=32,\n",
    "    head_num=4,\n",
    "    head_size=8,\n",
    "    layer_num=4,\n",
    "    ctoi=ctoi,\n",
    "    dropout=0\n",
    ")\n",
    "\n",
    "model = Transformer(cfg)\n",
    "optim = torch.optim.AdamW(model.parameters())\n",
    "sum([p.numel() for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "80b72bc5-6728-4c92-8e48-dd0a5ca9129b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6390573978424072"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.tensor(1/vocab_size).log().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d8b0095b-b9cd-4c77-ab44-336a6dcc68fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_valid(model):\n",
    "    model.eval()\n",
    "    x, y = random_batch(128, 1000) # (N, L)\n",
    "    logits = model(x)             # (N, L, vocab)\n",
    "    \n",
    "    # N, L = y\n",
    "    y = y[:,1:].reshape(-1)\n",
    "    logits = logits[:,:-1,:].reshape(-1, vocab_size)\n",
    "    loss = F.cross_entropy(logits, y, ignore_index=0)\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "51058ffa-d845-4379-bbc0-585ad4d1de9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6357204914093018 2.586203098297119\n",
      "1.29684579372406 3.768287181854248\n",
      "1.3459900617599487 4.3845672607421875\n",
      "1.158843994140625 3.6213533878326416\n",
      "0.8411932587623596 4.423673152923584\n",
      "0.7436173558235168 5.530391693115234\n",
      "0.36972683668136597 6.554106712341309\n",
      "0.1334693729877472 9.214505195617676\n",
      "0.1944161206483841 10.833005905151367\n",
      "0.027401503175497055 11.598226547241211\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for i in range(5000):\n",
    "    optim.zero_grad()\n",
    "    \n",
    "    x, y = random_batch(32) # (N, L)\n",
    "    logits = model(x)      # (N, L, vocab)\n",
    "    \n",
    "    # N, L = y\n",
    "    y = y[:,1:].reshape(-1)\n",
    "    logits = logits[:,:-1,:].reshape(-1, vocab_size)\n",
    "    \n",
    "    loss = F.cross_entropy(logits, y, ignore_index=0)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    if i % 500 == 0:\n",
    "        val_loss = eval_valid(model)\n",
    "        print(loss.item(), val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2d9a39-746f-4201-832a-2ea0e9c3e9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prob():\n",
    "    a = torch.randint(0, 100, (1,)).item()\n",
    "    b = torch.randint(0, 100, (1,)).item()\n",
    "    c = a + b\n",
    "    cs = f\"{c}\"[::-1]\n",
    "    return f\"{a}+{b}=\", c, cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35dd99a-548c-4377-8faa-b5daa5230afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_prob()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8fd96f-8316-4ffa-b4fb-5b937479fcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "for _ in range(10):\n",
    "    prob, solu, rsolu = generate_prob()\n",
    "    print(prob + str(solu))\n",
    "    \n",
    "    new_token = []\n",
    "    tokens = encode(prob)\n",
    "    for _ in range(100):\n",
    "        x = torch.tensor(tokens).view(1, -1) # (B, L)\n",
    "        logits = model(x)\n",
    "        last = logits[0,-1]\n",
    "        prob = F.softmax(last, dim=0)\n",
    "        ix = torch.multinomial(prob, 1).item()\n",
    "        \n",
    "        if ix == stop_token:\n",
    "            break\n",
    "            \n",
    "        new_token.append(ix)\n",
    "        tokens.append(ix)\n",
    "\n",
    "    s = \"\".join([itoc[t] for t in new_token])\n",
    "    s = s[::-1]\n",
    "    print(s)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50874fb8-db23-46ec-bbb5-2cbc1e0334a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
